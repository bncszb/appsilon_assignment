{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report of the assignment for my application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions\n",
    "\n",
    "#### 1. Strategy\n",
    "\n",
    "The goal was to create \"a quick proof of concept model verifying the feasibility of detecting flower species\". This task can be solved in more ways, for example objects can be detected by predicting bounding boxes or objects can be segmented semantically. \n",
    "\n",
    "I chose the latter, since the given dataset had segmentations and it seemed, that creating bounding boxes for each flower was a more complex task than choosing the right color in the images. I could have used the smallest bounding box containing all the segmentations, but there were images with flowers far away from each other, so I stayed with the segmentation. \n",
    "\n",
    "#### 2. Dataset\n",
    "\n",
    "The dataset I was presented with contained multiple splits, which I haven't used to makes things cleaner. I created my own dataset from the images which had segmentations with a train-test split of 80% and 20% each, after shuffling. This way the ratio of the species in the sets were more or less the same.\n",
    "\n",
    "Since I needed only one label for the flower I kept only the red color. This might not be the perfect way, because sometimes it seemed that keeping the black also would have resulted in better segmentation.\n",
    "\n",
    "#### 3. Preprocessig\n",
    "\n",
    "During preprocessing I resized all images to a square with (120, 120) pixels. I decided this way, because most of the images seemed to be approximately squares and having the same shape is better for the Tensorflow. Also this helped with the potential difference in resolution.\n",
    "\n",
    "This raised a small problem when the shape was smaller than (120,120), because the segmentations were padded with ones. This could be solved easily,but the training results are good enough to show the task is possible and unfortunately I don't have access to a gpu anymore.\n",
    "\n",
    "The images was also scaled to 0-1.\n",
    "\n",
    "#### 4. Model\n",
    "\n",
    "For my model I chose a simple Unet network which was shown to work well for human segmentation. I was in luck, it was great for flowers too :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The images shown are all from the test set. As you can see the predicted mask in the third column represents well the shape of the flower in the first column. Sometimes much better than the original segmentation.\n",
    "\n",
    "<img src=\"results.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "##### 1. How would you share your findings with the client?\n",
    "\n",
    "I would explain:\n",
    "- that segmenting an image is a harder problem than drawing bounding boxes. And even this can be achieved easily, so their product is possible.\n",
    "- that for the species identification is a classification problem, so beyond the segmented dataset we need enough example for each type of plant. The same is true for health of given plant.\n",
    "- that the other features of the product are part of an ordinary app development process.\n",
    "\n",
    "##### 2. What would your comments be to a colleague building the app, regarding the model?\n",
    "\n",
    "I would tell them about:\n",
    "- the small errors I made\n",
    "- how the preprocessing steps could be made mor sophisticated, like additional augmentation steps\n",
    "- that the architecture is not perfected for the task yet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback about the assignment\n",
    "\n",
    "\n",
    "I spent the most time at the prerocessing steps. Somehow transforming the pictures of the segmentations into binary masks wasn't intuitive for me. Also, deciding how to make a tensorflow dataset out of this and then creating it added to time spent here. So this was around 2-3 hours.\n",
    "\n",
    "Before that, exploring the task and the given dataset was around 0.5-1 hour. Researching how the task shoud be solved was more or less the same.\n",
    "\n",
    "The implementation of the model was fast, in half hour I had a trained model and had the plots for comparison.\n",
    "\n",
    "Altogether, it was more, around 7-8 hours, because I also needed to create a docker container to access the gpu of the research institute where I work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
